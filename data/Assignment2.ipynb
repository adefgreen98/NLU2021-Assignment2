{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMsxo4YcjY7Ovw1JTIXSHFu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adefgreen98/NLU2021-Assignment2/blob/main/data/Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbPe1_Qa2mDL"
      },
      "source": [
        "# Natural Language Understanding 2021 - Assignment 2: NERs & Dependency Parsing\n",
        "\n",
        "_Federico Pedeni, 223993_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNflApSWsuNU"
      },
      "source": [
        "### Current issues\n",
        "- During spaCy parsing of `test.txt`, the number of sentences grows from 3453 to 4205\n",
        "- Spacy has many more entity tags than the ground truth, it should be checked if they have statistical importance for the purpose of our study\n",
        "- Check what is the equivalent of ground truth's MISC (should be NORP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY_NwBjUGRjS"
      },
      "source": [
        "### Requirements\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kRn7qzwIuHj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4a6066c-16a2-4449-d00b-997fb946d84f"
      },
      "source": [
        "!git clone https://github.com/adefgreen98/NLU2021-Assignment2.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'NLU2021-Assignment2'...\n",
            "remote: Enumerating objects: 16, done.\u001b[K\n",
            "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 16 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (16/16), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4T3opOh295q"
      },
      "source": [
        "!git pull "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSMwml0tGTOm"
      },
      "source": [
        "import spacy\n",
        "import nltk\n",
        "import zipfile\n",
        "from asgnmt2_data.conll import *"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRzLNxaCHvJs"
      },
      "source": [
        "# Initialize parser\n",
        "nlp = spacy.load('en')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYswNObrNYSu"
      },
      "source": [
        "# Extract assignment data\n",
        "with zipfile.ZipFile(\"asgnmt2_data/conll2003.zip\") as zipref:\n",
        "    zipref.extractall('data')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXX04z9hF5NB"
      },
      "source": [
        "# Format of dataset: <TOKEN> <POS> <IOB part-of speech tag> <TAG>\n",
        "\n",
        "\n",
        "def load_dataset(mode):\n",
        "    res = {\n",
        "        'sentences': [],\n",
        "        'ners': []\n",
        "    }\n",
        "    pth = f'data/{mode}.txt'\n",
        "    \n",
        "    idx = 0\n",
        "\n",
        "    tmpsentence = []\n",
        "    tmpentity = {}\n",
        "    tmp_entities_in_sentence = []\n",
        "\n",
        "    tmpmisc = None\n",
        "\n",
        "    with open(pth, 'rt') as file:\n",
        "        for line in file:\n",
        "            idx += 1\n",
        "            if line == '\\n':\n",
        "                if len(tmpsentence) > 0:\n",
        "                    # adding artificial punctuation for only nominal sentences so that they are correclty parsed\n",
        "                    if tmpsentence[-1] != '.': \n",
        "                        tmpsentence.append('.')\n",
        "\n",
        "                    # flushes the current sentence\n",
        "                    res['sentences'].append(' '.join(tmpsentence[:-1]) + tmpsentence[-1])\n",
        "                    tmpsentence = []\n",
        "\n",
        "                    # flushes the last entity in entity list for sentence\n",
        "                    if len(tmpentity) > 0: tmp_entities_in_sentence.append(tmpentity)\n",
        "                    tmpentity = []\n",
        "                    \n",
        "                    # adds artificial punctuation if needed also to the entity list\n",
        "                    if tmp_entities_in_sentence[-1][-1][0] != '.':\n",
        "                        tmp_entities_in_sentence.append([('.', 'O')])\n",
        "                    \n",
        "                    # flushes entity list\n",
        "                    res['ners'].append(tmp_entities_in_sentence)\n",
        "                    tmp_entities_in_sentence = []\n",
        "                continue\n",
        "            elif line.startswith('-DOCSTART-'):\n",
        "                continue\n",
        "            else:\n",
        "                if len(line.split()) != 4: \n",
        "                    print(f\"Error: line with size {len(line.split())} at index {index}\")\n",
        "                token, pos, tag1, tag2 = line.split()\n",
        "                tmpsentence.append(token)\n",
        "\n",
        "                if tag2.startswith('B'):\n",
        "                    if len(tmpentity) > 0:\n",
        "                        tmp_entities_in_sentence.append(tmpentity)\n",
        "                        tmpentity = [(token, tag2)]\n",
        "                    else:\n",
        "                        tmpentity = [(token, tag2)]\n",
        "                elif tag2.startswith('I'):\n",
        "                    currtag = tag2.split('-')[1]\n",
        "                    oldtag =  tmpentity[-1][1].split('-')[1]\n",
        "                    if currtag != oldtag: \n",
        "                        raise RuntimeError(f\"not corresponding tags at index {idx}; tags are '{currtag}' (new) and '{oldtag}' (old)\")\n",
        "                    tmpentity.append((token, tag2))\n",
        "                elif tag2.startswith('O'): \n",
        "                    if len(tmpentity) > 0: tmp_entities_in_sentence.append(tmpentity)\n",
        "                    tmpentity = [(token, tag2)]\n",
        "                else:\n",
        "                    print(f\"Error: wrong tag detected at line {idx}, line: {line.encode()}\")\n",
        "\n",
        "    # TODO: solve the issue of MISC tagged-tokens that seem compound but appear without a subject (eg: 'German', 'British')\n",
        "    return res\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzVnFnbEQjRH"
      },
      "source": [
        "# Utility to return iob-tagging for a single string\n",
        "def tag_iob_string(sentence:str):\n",
        "    return [(token.text, \"-\".join([token.ent_iob_, token.ent_type_])) for token in nlp(sentence)]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fy02NlhJL1zl"
      },
      "source": [
        "### Named entity lanbels conversion from SpaCy format to CoNLL format\n",
        "Labelmaps converted to CoNLL according to [this](https://www.clips.uantwerpen.be/conll2003/ner/annotation.txt)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAeb10wJWasZ",
        "outputId": "2bb23581-d09a-40ff-f1b5-cbe5a38b2a9d"
      },
      "source": [
        "for el in nlp.entity.labels:\n",
        "    print(el, \": \", spacy.explain(el))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CARDINAL :  Numerals that do not fall under another type\n",
            "DATE :  Absolute or relative dates or periods\n",
            "EVENT :  Named hurricanes, battles, wars, sports events, etc.\n",
            "FAC :  Buildings, airports, highways, bridges, etc.\n",
            "GPE :  Countries, cities, states\n",
            "LANGUAGE :  Any named language\n",
            "LAW :  Named documents made into laws.\n",
            "LOC :  Non-GPE locations, mountain ranges, bodies of water\n",
            "MONEY :  Monetary values, including unit\n",
            "NORP :  Nationalities or religious or political groups\n",
            "ORDINAL :  \"first\", \"second\", etc.\n",
            "ORG :  Companies, agencies, institutions, etc.\n",
            "PERCENT :  Percentage, including \"%\"\n",
            "PERSON :  People, including fictional\n",
            "PRODUCT :  Objects, vehicles, foods, etc. (not services)\n",
            "QUANTITY :  Measurements, as of weight or distance\n",
            "TIME :  Times smaller than a day\n",
            "WORK_OF_ART :  Titles of books, songs, etc.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K620kf6_L0rE"
      },
      "source": [
        "\n",
        "# Labelmaps converted to CoNLL according to https://www.clips.uantwerpen.be/conll2003/ner/annotation.txt\n",
        "labelmap = {\n",
        "    'CARDINAL': 'out',\n",
        "    'DATE': 'out',\n",
        "    'EVENT': 'MISC',\n",
        "    'FAC': 'LOC',\n",
        "    'GPE': 'LOC',\n",
        "    'LANGUAGE': 'MISC',\n",
        "    'LAW': 'out',\n",
        "    'LOC': 'LOC',\n",
        "    'MONEY': 'out',\n",
        "    'NORP': 'MISC',\n",
        "    'ORDINAL': 'out',\n",
        "    'ORG': 'ORG',\n",
        "    'PERCENT': 'out',\n",
        "    'PERSON': 'PER',\n",
        "    'PRODUCT': 'out',\n",
        "    'QUANTITY': 'out',\n",
        "    'TIME': 'out',\n",
        "    'WORK_OF_ART': 'out',\n",
        "    '': 'out'\n",
        "}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUQ-GcvkOX3u"
      },
      "source": [
        "dataset = load_dataset('test')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnZYneSIWf-t",
        "outputId": "d1a228fd-bc27-4145-bc14-9a29b769038b"
      },
      "source": [
        "for sent, ents in zip(dataset['sentences'][:4], dataset['ners'][:4]):\n",
        "    print(\"Sentence: \", sent)\n",
        "    print(\"Ents: \", *ents, sep='\\n')\n",
        "    print(\"----------------------------\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence:  SOCCER - JAPAN GET LUCKY WIN , CHINA IN SURPRISE DEFEAT.\n",
            "Ents: \n",
            "[('SOCCER', 'O')]\n",
            "[('-', 'O')]\n",
            "[('JAPAN', 'B-LOC')]\n",
            "[('GET', 'O')]\n",
            "[('LUCKY', 'O')]\n",
            "[('WIN', 'O')]\n",
            "[(',', 'O')]\n",
            "[('CHINA', 'B-PER')]\n",
            "[('IN', 'O')]\n",
            "[('SURPRISE', 'O')]\n",
            "[('DEFEAT', 'O')]\n",
            "[('.', 'O')]\n",
            "----------------------------\n",
            "Sentence:  Nadim Ladki.\n",
            "Ents: \n",
            "[('Nadim', 'B-PER'), ('Ladki', 'I-PER')]\n",
            "[('.', 'O')]\n",
            "----------------------------\n",
            "Sentence:  AL-AIN , United Arab Emirates 1996-12-06.\n",
            "Ents: \n",
            "[('AL-AIN', 'B-LOC')]\n",
            "[(',', 'O')]\n",
            "[('United', 'B-LOC'), ('Arab', 'I-LOC'), ('Emirates', 'I-LOC')]\n",
            "[('1996-12-06', 'O')]\n",
            "[('.', 'O')]\n",
            "----------------------------\n",
            "Sentence:  Japan began the defence of their Asian Cup title with a lucky 2-1 win against Syria in a Group C championship match on Friday.\n",
            "Ents: \n",
            "[('Japan', 'B-LOC')]\n",
            "[('began', 'O')]\n",
            "[('the', 'O')]\n",
            "[('defence', 'O')]\n",
            "[('of', 'O')]\n",
            "[('their', 'O')]\n",
            "[('Asian', 'B-MISC'), ('Cup', 'I-MISC')]\n",
            "[('title', 'O')]\n",
            "[('with', 'O')]\n",
            "[('a', 'O')]\n",
            "[('lucky', 'O')]\n",
            "[('2-1', 'O')]\n",
            "[('win', 'O')]\n",
            "[('against', 'O')]\n",
            "[('Syria', 'B-LOC')]\n",
            "[('in', 'O')]\n",
            "[('a', 'O')]\n",
            "[('Group', 'O')]\n",
            "[('C', 'O')]\n",
            "[('championship', 'O')]\n",
            "[('match', 'O')]\n",
            "[('on', 'O')]\n",
            "[('Friday', 'O')]\n",
            "[('.', 'O')]\n",
            "----------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG7mczCu4eRP"
      },
      "source": [
        "### 1) Evaluate spaCy NER model using CoNLL evaluation script on CoNLL 2003 data \n",
        "+ report token-level performance (per class and total)\n",
        "> + accuracy of correctly recognizing all tokens that belong to named entities (i.e. tag-level accuracy)\n",
        "+ report CoNLL chunk-level performance (per class and total); \n",
        "> + precision, recall, f-measure of correctly recognizing all the named entities in a chunk per class and total\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJ_0KZ4oTv3g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5aa8bd5e-155f-4012-b583-f86d68c5d914"
      },
      "source": [
        "doc = nlp(dataset['sentences'][0])\n",
        "print(doc)\n",
        "print([(tk.text, tk.ent_iob_, tk.ent_type_) for tk in doc])\n",
        "print([[(tk.text, tk.ent_iob_, tk.ent_type_) for tk in nlp('japan')]])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SOCCER - JAPAN GET LUCKY WIN , CHINA IN SURPRISE DEFEAT.\n",
            "[('SOCCER', 'O', ''), ('-', 'O', ''), ('JAPAN', 'O', ''), ('GET', 'O', ''), ('LUCKY', 'O', ''), ('WIN', 'B', 'ORG'), (',', 'O', ''), ('CHINA', 'B', 'GPE'), ('IN', 'O', ''), ('SURPRISE', 'O', ''), ('DEFEAT', 'O', ''), ('.', 'O', '')]\n",
            "[[('japan', 'B', 'GPE')]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "Yw6v9HOwMgs1",
        "outputId": "df55d935-5902-4420-9057-ac5a9af8fb3c"
      },
      "source": [
        "metrics = {}\n",
        "\n",
        "for tag in {'MISC', 'ORG', 'LOC', 'PERS', 'out'}:\n",
        "    metrics[tag] = {\n",
        "        'tp': 0,\n",
        "        'tn': 0,\n",
        "        'fp': 0,\n",
        "        'fn': 0\n",
        "    }\n",
        "\n",
        "matching = 0\n",
        "total = 0\n",
        "\n",
        "# Computing ACCURACY sentence by sentence\n",
        "for sent, ents in zip(dataset['sentences'][:20], dataset['ners'][:20]):\n",
        "    doc = nlp(sent)\n",
        "    for tk in doc:\n",
        "        conv_label = tk.ent_iob_ + (('-' + labelmap[tk.ent_type_]) if labelmap[tk.ent_type_] != 'out' else '')\n",
        "        idx = [enttoken[0] for ent in ents for enttoken in ent].index(tk.text)\n",
        "        labels = [enttoken[1] for ent in ents for enttoken in ent]\n",
        "        if conv_label == labels[idx]:\n",
        "            matching += 1\n",
        "        total += 1\n",
        "\n",
        "    print(\"Matching: \", matching, \"  |  \", \"Total: \", total)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matching:  9   |   Total:  12\n",
            "Matching:  10   |   Total:  15\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-5b83038a43c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mconv_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ment_iob_\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlabelmap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ment_type_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlabelmap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ment_type_\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'out'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0menttoken\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ments\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0menttoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0menttoken\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ments\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0menttoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconv_label\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: 'AL' is not in list"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shrNV0XsF4b6"
      },
      "source": [
        "\n",
        "### 2) Grouping of Entities. Write a function to group recognized named entities using noun_chunks method of spaCy. Analyze the groups in terms of most frequent combinations (i.e. NER types that go together).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkEtp8oIF8_1"
      },
      "source": [
        "### 3) One of the possible post-processing steps is to fix segmentation errors. Write a function that extends the entity span to cover the full noun-compounds. Make use of compound dependency relation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1tETWyQyaiD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nad8BE_UybEF"
      },
      "source": [
        "### Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20DDy7KyOk6n"
      },
      "source": [
        "str_dataset = \" \".join([sent for sent in dataset['sentences']])\n",
        "print(*str_dataset.split('.')[:5], sep='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kg6Ys9RSyqGg"
      },
      "source": [
        "doc = nlp(str_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EI-6K6dQpshn"
      },
      "source": [
        "entities_counts = {'total': 0}\n",
        "for sent in doc.sents:\n",
        "    for tk in sent:\n",
        "        if tk.ent_type_ == 'NORP' and entities_counts['total'] < 1000:\n",
        "            print(tk)\n",
        "        try: entities_counts[tk.ent_type_] += 1\n",
        "        except KeyError: entities_counts[tk.ent_type_] = 1\n",
        "        entities_counts['total'] += 1\n",
        "print(*entities_counts.items(), sep='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDquLvZmrhSR"
      },
      "source": [
        "print(\"After Parsing: \", len(list(doc.sents)))\n",
        "print(\"Before Parsing: \", len(dataset['sentences']))\n",
        "print(\"From read_corpus_conll(): \", len(read_corpus_conll('/content/data/test.txt')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Pq8rE9aNjtx"
      },
      "source": [
        "# here i show that my loading method works perfectly\n",
        "i = 0\n",
        "for sent in read_corpus_conll('/content/data/test.txt'):\n",
        "    if sent[0][0] == '-DOCSTART- -X- -X- O':\n",
        "        i += 1\n",
        "print(i)\n",
        "print(len(read_corpus_conll('/content/data/test.txt')) - len(dataset['sentences']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUTn-6RbIrxu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}