{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPSbv0Wy3FiaMOJxAcX6PMH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adefgreen98/NLU2021-Assignment2/blob/main/code/Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbPe1_Qa2mDL"
      },
      "source": [
        "# Natural Language Understanding 2021 - Assignment 2: NERs & Dependency Parsing\n",
        "\n",
        "_Federico Pedeni, 223993_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNflApSWsuNU"
      },
      "source": [
        "### Current issues\n",
        "- During spaCy parsing of `test.txt`, the number of sentences grows from 3453 to 4205\n",
        "- Spacy has many more entity tags than the ground truth, it should be checked if they have statistical importance for the purpose of our study\n",
        "- Check what is the equivalent of ground truth's MISC (should be NORP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY_NwBjUGRjS"
      },
      "source": [
        "### Requirements\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kRn7qzwIuHj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f48d6a9-edcb-49e9-bee3-9c3470a28acd"
      },
      "source": [
        "!git clone https://github.com/adefgreen98/NLU2021-Assignment2.git\n",
        "!mv /content/NLU2021-Assignment2/code/conll.py ./"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'NLU2021-Assignment2' already exists and is not an empty directory.\n",
            "mv: cannot stat '/content/NLU2021-Assignment2/code/conll.py': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSMwml0tGTOm"
      },
      "source": [
        "import spacy\n",
        "import nltk\n",
        "import zipfile\n",
        "import re\n",
        "from conll import *"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRzLNxaCHvJs"
      },
      "source": [
        "# Initialize parser\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "# correct tokenizer to use only whitespaces\n",
        "nlp.tokenizer = lambda input: spacy.tokens.Doc(nlp.vocab, input.split()[:-1] + (['.'] if input[-1] == '.' else []))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYswNObrNYSu"
      },
      "source": [
        "# Extract assignment data\n",
        "with zipfile.ZipFile(\"/content/NLU2021-Assignment2/data/conll2003.zip\") as zipref:\n",
        "    zipref.extractall('data')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXX04z9hF5NB"
      },
      "source": [
        "# Format of dataset: <TOKEN> <POS> <IOB part-of speech tag> <TAG>\n",
        "\n",
        "\n",
        "def load_dataset(mode):\n",
        "    res = {\n",
        "        'sentences': [],\n",
        "        'ners': []\n",
        "    }\n",
        "    pth = f'data/{mode}.txt'\n",
        "    \n",
        "    idx = 0\n",
        "\n",
        "    tmpsentence = []\n",
        "    tmpentity = {}\n",
        "    tmp_entities_in_sentence = []\n",
        "\n",
        "    tmpmisc = None\n",
        "\n",
        "    with open(pth, 'rt') as file:\n",
        "        for line in file:\n",
        "            idx += 1\n",
        "            if line == '\\n':\n",
        "                if len(tmpsentence) > 0:\n",
        "                    # adding artificial punctuation for only nominal sentences so that they are correclty parsed\n",
        "                    if tmpsentence[-1] != '.': \n",
        "                        tmpsentence.append('.')\n",
        "\n",
        "                    # flushes the current sentence\n",
        "                    res['sentences'].append(' '.join(tmpsentence[:-1]) + tmpsentence[-1])\n",
        "                    tmpsentence = []\n",
        "\n",
        "                    # flushes the last entity in entity list for sentence\n",
        "                    if len(tmpentity) > 0: tmp_entities_in_sentence.append(tmpentity)\n",
        "                    tmpentity = []\n",
        "                    \n",
        "                    # adds artificial punctuation if needed also to the entity list\n",
        "                    if tmp_entities_in_sentence[-1][-1][0] != '.':\n",
        "                        tmp_entities_in_sentence.append([('.', 'O')])\n",
        "                    \n",
        "                    # flushes entity list\n",
        "                    res['ners'].append(tmp_entities_in_sentence)\n",
        "                    tmp_entities_in_sentence = []\n",
        "                continue\n",
        "            elif line.startswith('-DOCSTART-'):\n",
        "                continue\n",
        "            else:\n",
        "                if len(line.split()) != 4: \n",
        "                    print(f\"Error: line with size {len(line.split())} at index {index}\")\n",
        "                token, pos, tag1, tag2 = line.split()\n",
        "                tmpsentence.append(token)\n",
        "\n",
        "                if tag2.startswith('B'):\n",
        "                    if len(tmpentity) > 0:\n",
        "                        tmp_entities_in_sentence.append(tmpentity)\n",
        "                        tmpentity = [(token, tag2)]\n",
        "                    else:\n",
        "                        tmpentity = [(token, tag2)]\n",
        "                elif tag2.startswith('I'):\n",
        "                    currtag = tag2.split('-')[1]\n",
        "                    oldtag =  tmpentity[-1][1].split('-')[1]\n",
        "                    if currtag != oldtag: \n",
        "                        raise RuntimeError(f\"not corresponding tags at index {idx}; tags are '{currtag}' (new) and '{oldtag}' (old)\")\n",
        "                    tmpentity.append((token, tag2))\n",
        "                elif tag2.startswith('O'): \n",
        "                    if len(tmpentity) > 0: tmp_entities_in_sentence.append(tmpentity)\n",
        "                    tmpentity = [(token, tag2)]\n",
        "                else:\n",
        "                    print(f\"Error: wrong tag detected at line {idx}, line: {line.encode()}\")\n",
        "\n",
        "    # TODO: solve the issue of MISC tagged-tokens that seem compound but appear without a subject (eg: 'German', 'British')\n",
        "    return res\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzVnFnbEQjRH"
      },
      "source": [
        "# Utility to return iob-tagging for a single string\n",
        "def tag_iob_string(sentence:str):\n",
        "    return [(token.text, \"-\".join([token.ent_iob_, token.ent_type_])) for token in nlp(sentence)]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fy02NlhJL1zl"
      },
      "source": [
        "### Named entity lanbels conversion from SpaCy format to CoNLL format\n",
        "Labelmaps converted to CoNLL according to [this](https://www.clips.uantwerpen.be/conll2003/ner/annotation.txt)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAeb10wJWasZ",
        "outputId": "620f772e-1e3b-4193-f636-0e565d47e313"
      },
      "source": [
        "for el in nlp.entity.labels:\n",
        "    print(el, \": \", spacy.explain(el))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CARDINAL :  Numerals that do not fall under another type\n",
            "DATE :  Absolute or relative dates or periods\n",
            "EVENT :  Named hurricanes, battles, wars, sports events, etc.\n",
            "FAC :  Buildings, airports, highways, bridges, etc.\n",
            "GPE :  Countries, cities, states\n",
            "LANGUAGE :  Any named language\n",
            "LAW :  Named documents made into laws.\n",
            "LOC :  Non-GPE locations, mountain ranges, bodies of water\n",
            "MONEY :  Monetary values, including unit\n",
            "NORP :  Nationalities or religious or political groups\n",
            "ORDINAL :  \"first\", \"second\", etc.\n",
            "ORG :  Companies, agencies, institutions, etc.\n",
            "PERCENT :  Percentage, including \"%\"\n",
            "PERSON :  People, including fictional\n",
            "PRODUCT :  Objects, vehicles, foods, etc. (not services)\n",
            "QUANTITY :  Measurements, as of weight or distance\n",
            "TIME :  Times smaller than a day\n",
            "WORK_OF_ART :  Titles of books, songs, etc.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K620kf6_L0rE"
      },
      "source": [
        "\n",
        "# Labelmaps converted to CoNLL according to https://www.clips.uantwerpen.be/conll2003/ner/annotation.txt\n",
        "labelmap = {\n",
        "    'CARDINAL': 'out',\n",
        "    'DATE': 'out',\n",
        "    'EVENT': 'MISC',\n",
        "    'FAC': 'LOC',\n",
        "    'GPE': 'LOC',\n",
        "    'LANGUAGE': 'MISC',\n",
        "    'LAW': 'out',\n",
        "    'LOC': 'LOC',\n",
        "    'MONEY': 'out',\n",
        "    'NORP': 'MISC',\n",
        "    'ORDINAL': 'out',\n",
        "    'ORG': 'ORG',\n",
        "    'PERCENT': 'out',\n",
        "    'PERSON': 'PER',\n",
        "    'PRODUCT': 'out',\n",
        "    'QUANTITY': 'out',\n",
        "    'TIME': 'out',\n",
        "    'WORK_OF_ART': 'out',\n",
        "    '': 'out'\n",
        "}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUQ-GcvkOX3u"
      },
      "source": [
        "dataset = load_dataset('test')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnZYneSIWf-t",
        "outputId": "58c3da21-0942-45d8-a914-bafd894a45f1"
      },
      "source": [
        "for sent, ents in zip(dataset['sentences'][:4], dataset['ners'][:4]):\n",
        "    print(\"Sentence: \", sent)\n",
        "    print(\"Ents: \", *ents, sep='\\n')\n",
        "    print(\"----------------------------\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence:  SOCCER - JAPAN GET LUCKY WIN , CHINA IN SURPRISE DEFEAT.\n",
            "Ents: \n",
            "[('SOCCER', 'O')]\n",
            "[('-', 'O')]\n",
            "[('JAPAN', 'B-LOC')]\n",
            "[('GET', 'O')]\n",
            "[('LUCKY', 'O')]\n",
            "[('WIN', 'O')]\n",
            "[(',', 'O')]\n",
            "[('CHINA', 'B-PER')]\n",
            "[('IN', 'O')]\n",
            "[('SURPRISE', 'O')]\n",
            "[('DEFEAT', 'O')]\n",
            "[('.', 'O')]\n",
            "----------------------------\n",
            "Sentence:  Nadim Ladki.\n",
            "Ents: \n",
            "[('Nadim', 'B-PER'), ('Ladki', 'I-PER')]\n",
            "[('.', 'O')]\n",
            "----------------------------\n",
            "Sentence:  AL-AIN , United Arab Emirates 1996-12-06.\n",
            "Ents: \n",
            "[('AL-AIN', 'B-LOC')]\n",
            "[(',', 'O')]\n",
            "[('United', 'B-LOC'), ('Arab', 'I-LOC'), ('Emirates', 'I-LOC')]\n",
            "[('1996-12-06', 'O')]\n",
            "[('.', 'O')]\n",
            "----------------------------\n",
            "Sentence:  Japan began the defence of their Asian Cup title with a lucky 2-1 win against Syria in a Group C championship match on Friday.\n",
            "Ents: \n",
            "[('Japan', 'B-LOC')]\n",
            "[('began', 'O')]\n",
            "[('the', 'O')]\n",
            "[('defence', 'O')]\n",
            "[('of', 'O')]\n",
            "[('their', 'O')]\n",
            "[('Asian', 'B-MISC'), ('Cup', 'I-MISC')]\n",
            "[('title', 'O')]\n",
            "[('with', 'O')]\n",
            "[('a', 'O')]\n",
            "[('lucky', 'O')]\n",
            "[('2-1', 'O')]\n",
            "[('win', 'O')]\n",
            "[('against', 'O')]\n",
            "[('Syria', 'B-LOC')]\n",
            "[('in', 'O')]\n",
            "[('a', 'O')]\n",
            "[('Group', 'O')]\n",
            "[('C', 'O')]\n",
            "[('championship', 'O')]\n",
            "[('match', 'O')]\n",
            "[('on', 'O')]\n",
            "[('Friday', 'O')]\n",
            "[('.', 'O')]\n",
            "----------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG7mczCu4eRP"
      },
      "source": [
        "### 1) Evaluate spaCy NER model using CoNLL evaluation script on CoNLL 2003 data \n",
        "+ report token-level performance (per class and total)\n",
        "> + accuracy of correctly recognizing all tokens that belong to named entities (i.e. tag-level accuracy)\n",
        "+ report CoNLL chunk-level performance (per class and total); \n",
        "> + precision, recall, f-measure of correctly recognizing all the named entities in a chunk per class and total\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw6v9HOwMgs1",
        "outputId": "ffacc5be-bd2f-45a7-bcf7-9478fa4c866e"
      },
      "source": [
        "# Part 1\n",
        "import itertools \n",
        "\n",
        "accuracies = {k: {'matching': 0, 'total': 0} for k in [el[0] + el[1] for el in itertools.product(['B-', 'I-'], ['ORG', 'PER', 'LOC', 'MISC'])] + ['O']}\n",
        "\n",
        "# Computing ACCURACY sentence by sentence\n",
        "for sent, ents in zip(dataset['sentences'], dataset['ners']):\n",
        "    doc = nlp(sent)\n",
        "    for tk in doc:\n",
        "        # label converted according to CoNLL standard (if it is not 'O')\n",
        "        conv_label = (tk.ent_iob_ + ('-' + labelmap[tk.ent_type_])) if labelmap[tk.ent_type_] != 'out' else 'O'\n",
        "        # tokens in a sentence (to get the index)\n",
        "        sent_tks = [enttoken[0] for ent in ents for enttoken in ent]\n",
        "        idx = sent_tks.index(tk.text)\n",
        "        # finds corresponding ground truth\n",
        "        labels = [enttoken[1] for ent in ents for enttoken in ent]\n",
        "        if conv_label == labels[idx]:\n",
        "            accuracies[conv_label]['matching'] += 1\n",
        "        accuracies[conv_label]['total'] += 1\n",
        "\n",
        "for cls, vals in accuracies.items():\n",
        "    print(f\"Accuracy for {cls}: {vals['matching'] / vals['total']}\")\n",
        "\n",
        "global_accuracy = sum([d['matching'] for d in accuracies.values()]) / sum([d['total'] for d in accuracies.values()])\n",
        "print(f\"Global Accuracy: {global_accuracy}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy for B-ORG: 0.4899267399267399\n",
            "Accuracy for B-PER: 0.7568659127625202\n",
            "Accuracy for B-LOC: 0.7636986301369864\n",
            "Accuracy for B-MISC: 0.796976241900648\n",
            "Accuracy for I-ORG: 0.4116485686080948\n",
            "Accuracy for I-PER: 0.679635761589404\n",
            "Accuracy for I-LOC: 0.4825174825174825\n",
            "Accuracy for I-MISC: 0.6166666666666667\n",
            "Accuracy for O: 0.9578903042425518\n",
            "Global Accuracy: 0.9092165077101346\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6GJPq_bYrPB"
      },
      "source": [
        "# Part 2\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shrNV0XsF4b6"
      },
      "source": [
        "\n",
        "### 2) Grouping of Entities. Write a function to group recognized named entities using noun_chunks method of spaCy. Analyze the groups in terms of most frequent combinations (i.e. NER types that go together).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foYmQvUVUWN4"
      },
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkEtp8oIF8_1"
      },
      "source": [
        "### 3) One of the possible post-processing steps is to fix segmentation errors. Write a function that extends the entity span to cover the full noun-compounds. Make use of compound dependency relation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1tETWyQyaiD"
      },
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nad8BE_UybEF"
      },
      "source": [
        "### Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20DDy7KyOk6n",
        "outputId": "5eb59fa8-72a8-4c0b-9a08-bc9e29b509a1"
      },
      "source": [
        "str_dataset = \" \".join([sent for sent in dataset['sentences']])\n",
        "print(*str_dataset.split('.')[:5], sep='\\n')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SOCCER - JAPAN GET LUCKY WIN , CHINA IN SURPRISE DEFEAT\n",
            " Nadim Ladki\n",
            " AL-AIN , United Arab Emirates 1996-12-06\n",
            " Japan began the defence of their Asian Cup title with a lucky 2-1 win against Syria in a Group C championship match on Friday\n",
            " But China saw their luck desert them in the second match of the group , crashing to a surprise 2-0 defeat to newcomers Uzbekistan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kg6Ys9RSyqGg"
      },
      "source": [
        "doc = nlp(str_dataset)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EI-6K6dQpshn",
        "outputId": "d925ccdc-f53a-4a61-a79c-15d8b9e294cd"
      },
      "source": [
        "entities_counts = {'total': 0}\n",
        "for sent in doc.sents:\n",
        "    for tk in sent:\n",
        "        if tk.ent_type_ == 'NORP' and entities_counts['total'] < 1000:\n",
        "            print(tk)\n",
        "        try: entities_counts[tk.ent_type_] += 1\n",
        "        except KeyError: entities_counts[tk.ent_type_] = 1\n",
        "        entities_counts['total'] += 1\n",
        "print(*entities_counts.items(), sep='\\n')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Chinese\n",
            "Soviet\n",
            "Syrian\n",
            "Syrian\n",
            "Syrian\n",
            "Syrian\n",
            "Syrians\n",
            "Marcello\n",
            "Syrian\n",
            "Syrian\n",
            "('total', 44876)\n",
            "('', 33377)\n",
            "('ORG', 2080)\n",
            "('GPE', 1579)\n",
            "('EVENT', 195)\n",
            "('ORDINAL', 138)\n",
            "('TIME', 253)\n",
            "('PERSON', 2339)\n",
            "('NORP', 421)\n",
            "('DATE', 1389)\n",
            "('CARDINAL', 1798)\n",
            "('FAC', 103)\n",
            "('LOC', 128)\n",
            "('QUANTITY', 226)\n",
            "('PRODUCT', 166)\n",
            "('MONEY', 347)\n",
            "('PERCENT', 200)\n",
            "('WORK_OF_ART', 74)\n",
            "('LAW', 61)\n",
            "('LANGUAGE', 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDquLvZmrhSR",
        "outputId": "f7f342eb-0ca7-4bde-a0c7-1af3a2c33820"
      },
      "source": [
        "print(\"After Parsing: \", len(list(doc.sents)))\n",
        "print(\"Before Parsing: \", len(dataset['sentences']))\n",
        "print(\"From read_corpus_conll(): \", len(read_corpus_conll('/content/data/test.txt')))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After Parsing:  3454\n",
            "Before Parsing:  3453\n",
            "From read_corpus_conll():  3684\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Pq8rE9aNjtx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80903f17-e5ab-461f-b6a1-8c41266eaa27"
      },
      "source": [
        "# here i show that my loading method works perfectly\n",
        "i = 0\n",
        "for sent in read_corpus_conll('/content/data/test.txt'):\n",
        "    if sent[0][0] == '-DOCSTART- -X- -X- O':\n",
        "        i += 1\n",
        "print(i)\n",
        "print(len(read_corpus_conll('/content/data/test.txt')) - len(dataset['sentences']))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "231\n",
            "231\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUTn-6RbIrxu"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    }
  ]
}