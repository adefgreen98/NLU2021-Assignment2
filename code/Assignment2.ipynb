{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1oGCzl0qwhCkFwj442tOaql2A8pnnYVBi",
      "authorship_tag": "ABX9TyOK7vpCAQw1OqtGM6WcZSx+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adefgreen98/NLU2021-Assignment2/blob/main/code/Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbPe1_Qa2mDL"
      },
      "source": [
        "# Natural Language Understanding 2021 - Assignment 2: NERs & Dependency Parsing\n",
        "\n",
        "_Federico Pedeni, 223993_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNflApSWsuNU"
      },
      "source": [
        "### Current issues\n",
        "- During spaCy parsing of `test.txt`, the number of sentences grows from 3453 to 4205\n",
        "- Spacy has many more entity tags than the ground truth, it should be checked if they have statistical importance for the purpose of our study\n",
        "- Check what is the equivalent of ground truth's MISC (should be NORP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY_NwBjUGRjS"
      },
      "source": [
        "### Requirements\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kRn7qzwIuHj"
      },
      "source": [
        "!cp -R \"drive/MyDrive/Colab Notebooks/NLU/asgnmt2_data/\" ./"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSMwml0tGTOm"
      },
      "source": [
        "import spacy\n",
        "import nltk\n",
        "import zipfile\n",
        "from asgnmt2_data.conll import *"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRzLNxaCHvJs"
      },
      "source": [
        "# Initialize parser\n",
        "nlp = spacy.load('en')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYswNObrNYSu"
      },
      "source": [
        "# Extract assignment data\n",
        "with zipfile.ZipFile(\"asgnmt2_data/conll2003.zip\") as zipref:\n",
        "    zipref.extractall('data')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXX04z9hF5NB"
      },
      "source": [
        "# Format of dataset: <TOKEN> <POS> <IOB part-of speech tag> <TAG>\n",
        "\n",
        "\n",
        "def load_dataset(mode):\n",
        "    res = {\n",
        "        'sentences': [],\n",
        "        'ners': {}\n",
        "    }\n",
        "    pth = f'data/{mode}.txt'\n",
        "    \n",
        "    idx = 0\n",
        "\n",
        "    tmpsentence = []\n",
        "    tmpentity = []\n",
        "\n",
        "    tmpmisc = None\n",
        "\n",
        "    with open(pth, 'rt') as file:\n",
        "        for line in file:\n",
        "            idx += 1\n",
        "            if line == '\\n':\n",
        "                if len(tmpsentence) > 0:\n",
        "                # flushes the current sentence\n",
        "                    # creates spaced sentence but the last word must be stacked with punctuation\n",
        "                    # also: adding artificial punctuation for only nominal sentences so that they are correclty parsed\n",
        "                    if tmpsentence[-1] != '.': tmpsentence.append('.')\n",
        "                    res['sentences'].append(' '.join(tmpsentence[:-1]) + tmpsentence[-1])\n",
        "                    tmpsentence = []\n",
        "                continue\n",
        "            elif line.startswith('-DOCSTART-'):\n",
        "                continue\n",
        "            else:\n",
        "                if len(line.split()) != 4: \n",
        "                    print(f\"Error: line with size {len(line.split())} at index {index}\")\n",
        "                token, pos, tag1, tag2 = line.split()\n",
        "                tmpsentence.append(token)\n",
        "                if tag2.startswith('B'):\n",
        "                    if len(tmpentity) > 0:\n",
        "                        currtag = tmpentity[-1][1].split('-')[1]\n",
        "                        try: res['ners'][currtag].append(tmpentity)\n",
        "                        except KeyError: res['ners'][currtag] = [tmpentity]\n",
        "                        tmpentity = [(token, tag2)]\n",
        "                    else:\n",
        "                        tmpentity = [(token, tag2)]\n",
        "                elif tag2.startswith('I'):\n",
        "                    tmpentity.append((token, tag2))\n",
        "                elif tag2.startswith('O'): \n",
        "                    try: res['ners']['O'].append(token)\n",
        "                    except KeyError: res['ners']['O'] = [token]\n",
        "                else:\n",
        "                    print(f\"Error: wrong tag detected at line {idx}, line: {line.encode()}\")\n",
        "\n",
        "    # TODO: solve the issue of MISC tagged-tokens that seem compound but appear without a subject (eg: 'German', 'British')\n",
        "    return res\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzVnFnbEQjRH"
      },
      "source": [
        "# Utility to return iob-tagging for a single string\n",
        "def tag_iob_string(sentence:str):\n",
        "    return [(token.text, \"-\".join([token.ent_iob_, token.ent_type_])) for token in nlp(sentence)]\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUQ-GcvkOX3u"
      },
      "source": [
        "dataset = load_dataset('test')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnZYneSIWf-t",
        "outputId": "14f175ec-5dd6-4425-f32d-4d040bf1092c"
      },
      "source": [
        "for k,v in dataset['ners'].items():\n",
        "    print(k, \": \", v[:4])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O :  ['SOCCER', '-', 'GET', 'LUCKY']\n",
            "LOC :  [[('JAPAN', 'B-LOC')], [('AL-AIN', 'B-LOC')], [('United', 'B-LOC'), ('Arab', 'I-LOC'), ('Emirates', 'I-LOC')], [('Japan', 'B-LOC')]]\n",
            "PER :  [[('CHINA', 'B-PER')], [('Nadim', 'B-PER'), ('Ladki', 'I-PER')], [('Igor', 'B-PER'), ('Shkvyrin', 'I-PER')], [('Oleg', 'B-PER'), ('Shatskiku', 'I-PER')]]\n",
            "MISC :  [[('Asian', 'B-MISC'), ('Cup', 'I-MISC')], [('Uzbek', 'B-MISC')], [('Chinese', 'B-MISC')], [('Soviet', 'B-MISC')]]\n",
            "ORG :  [[('FIFA', 'B-ORG')], [('RUGBY', 'B-ORG'), ('UNION', 'I-ORG')], [('Plymouth', 'B-ORG')], [('Exeter', 'B-ORG')]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG7mczCu4eRP"
      },
      "source": [
        "### 1) Evaluate spaCy NER model using CoNLL evaluation script on CoNLL 2003 data \n",
        "+ report token-level performance (per class and total)\n",
        "> + accuracy of correctly recognizing all tokens that belong to named entities (i.e. tag-level accuracy)\n",
        "+ report CoNLL chunk-level performance (per class and total); \n",
        "> + precision, recall, f-measure of correctly recognizing all the named entities in a chunk per class and total\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJ_0KZ4oTv3g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85642a96-7251-43d2-9eaa-e4a80afe8ae9"
      },
      "source": [
        "doc = nlp(dataset['sentences'][0])\n",
        "print(doc)\n",
        "print([(tk.text, tk.ent_iob_, tk.ent_type_) for tk in doc])\n",
        "print([[(tk.text, tk.ent_iob_, tk.ent_type_) for tk in nlp('japan')]])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SOCCER - JAPAN GET LUCKY WIN , CHINA IN SURPRISE DEFEAT.\n",
            "[('SOCCER', 'O', ''), ('-', 'O', ''), ('JAPAN', 'O', ''), ('GET', 'O', ''), ('LUCKY', 'O', ''), ('WIN', 'B', 'ORG'), (',', 'O', ''), ('CHINA', 'B', 'GPE'), ('IN', 'O', ''), ('SURPRISE', 'O', ''), ('DEFEAT', 'O', ''), ('.', 'O', '')]\n",
            "[[('japan', 'B', 'GPE')]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shrNV0XsF4b6"
      },
      "source": [
        "\n",
        "### 2) Grouping of Entities. Write a function to group recognized named entities using noun_chunks method of spaCy. Analyze the groups in terms of most frequent combinations (i.e. NER types that go together).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkEtp8oIF8_1"
      },
      "source": [
        "### 3) One of the possible post-processing steps is to fix segmentation errors. Write a function that extends the entity span to cover the full noun-compounds. Make use of compound dependency relation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1tETWyQyaiD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nad8BE_UybEF"
      },
      "source": [
        "### Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20DDy7KyOk6n"
      },
      "source": [
        "str_dataset = \" \".join([sent for sent in dataset['sentences']])\n",
        "print(*str_dataset.split('.')[:5], sep='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kg6Ys9RSyqGg"
      },
      "source": [
        "doc = nlp(str_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EI-6K6dQpshn"
      },
      "source": [
        "entities_counts = {'total': 0}\n",
        "for sent in doc.sents:\n",
        "    for tk in sent:\n",
        "        if tk.ent_type_ == 'NORP' and entities_counts['total'] < 1000:\n",
        "            print(tk)\n",
        "        try: entities_counts[tk.ent_type_] += 1\n",
        "        except KeyError: entities_counts[tk.ent_type_] = 1\n",
        "        entities_counts['total'] += 1\n",
        "print(*entities_counts.items(), sep='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDquLvZmrhSR"
      },
      "source": [
        "print(\"After Parsing: \", len(list(doc.sents)))\n",
        "print(\"Before Parsing: \", len(dataset['sentences']))\n",
        "print(\"From read_corpus_conll(): \", len(read_corpus_conll('/content/data/test.txt')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Pq8rE9aNjtx"
      },
      "source": [
        "# here i show that my loading method works perfectly\n",
        "i = 0\n",
        "for sent in read_corpus_conll('/content/data/test.txt'):\n",
        "    if sent[0][0] == '-DOCSTART- -X- -X- O':\n",
        "        i += 1\n",
        "print(i)\n",
        "print(len(read_corpus_conll('/content/data/test.txt')) - len(dataset['sentences']))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}